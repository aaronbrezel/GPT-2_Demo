{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trump_demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBPceeQaxr9y",
        "colab_type": "text"
      },
      "source": [
        "# Demoing a GPT-2 model finetuned on 28,000 tweets from the @realdonaldtrump twitter account\n",
        "\n",
        "This notebook demontrates how to build a bot around a finetuned GPT-2 language model. The code below is similar to what underpins the [roboTrump Twitter bot](https://twitter.com/RoboTrump3).\n",
        "This notebook is also intended for [Google Colab](https://colab.research.google.com/). Some commands may not work elsewhere.\n", 
        "To run this notebook, you need a GPT-2 model. You can finetune one yourself using the [Trump_finetune.ipynb](https://github.com/aaronbrezel/GPT-2_Demo/blob/master/Trump_finetune.ipynb). Or you can use a pretrained model by downloading this [checkpoint folder](https://github.com/aaronbrezel/GPT-2_Demo/tree/master/bot/checkpoint) and uploading it to your Google Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6LGDpcr0lew",
        "colab_type": "text"
      },
      "source": [
        "## Set up coding environment to run the bot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vkei06g0sXG",
        "colab_type": "text"
      },
      "source": [
        "This is same process we used in the finetuning notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4H99Ay0xULm",
        "colab_type": "code",
        "outputId": "dd76ec58-043d-4c64-b8cf-76b93cf58249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▌                               | 10kB 25.7MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 30.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 38.2MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 6.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 7.7MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 9.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 10.4MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 11.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 194kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 215kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 235kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 266kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 286kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 307kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 317kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 337kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 358kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 378kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 389kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 399kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 409kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 430kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 440kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 450kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 460kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 471kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 481kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 501kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 512kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 522kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 532kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 542kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 552kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 563kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 573kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 593kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 614kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 634kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 14.0MB/s \n",
            "\u001b[?25h  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SMwsekI1Hct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ <= \"2.0\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS0yrOTy0xEb",
        "colab_type": "text"
      },
      "source": [
        "Mount your gdrive where the .tar model checkpoint is stored. This allows us to seamlessly load the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIP1Ts2I1cAe",
        "colab_type": "code",
        "outputId": "52623ff3-1e67-474f-d3b9-116eb1c3f9cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #For the record, there is also a method in gpt-2-simple that does this, but it's the same action"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frPGgXMl2M9Z",
        "colab_type": "text"
      },
      "source": [
        "The next code snippet copies and unzips the .tar file from you drive. It creates a \"checkpoint\" folder. Inside is another folder named for the run_name that you established in the previous notebook. Inside that is all the files that you need to run the generate method. With them, we can make the bot talk.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5R8bdkM1qyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name=\"trump_tune_small\") #Use the same run name you picked in the previous notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0paofH4L3JmW",
        "colab_type": "text"
      },
      "source": [
        "Establish a gpt-2 session and load the run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAyXPnID3MvO",
        "colab_type": "code",
        "outputId": "9c82ba76-e767-4462-fc7d-7afe3bf0e9f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name=\"trump_tune_small\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/trump_tune_small/model-400\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/trump_tune_small/model-400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS_nwC4g4HO2",
        "colab_type": "text"
      },
      "source": [
        "### Set methods for running the bot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blo2HCmg4ecn",
        "colab_type": "text"
      },
      "source": [
        "gpt_predict will be our method for generating text off a prompt and cleaning it up to look like a tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3aqe2Yp4Kuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "def gpt_predict(prompt):\n",
        "\n",
        "  gen_text = gpt2.generate(sess, length=250, temperature=0.7, prefix=prompt, run_name='trump_tune_small', return_as_list=True)[0]\n",
        "\n",
        "  text_minus_tags = str(gen_text).replace(\"<|endoftext|>\",\"\").replace(\"<|startoftext|>\", \"\") #remove text artifacts from gpt-2 output\n",
        "\n",
        "  short_tweet_text = shorten_output(text_minus_tags) #shorten output of GPT-2 into a useable tweet length \n",
        "  short_tweet_tokens = remove_outbound_links_and_blank_tokens(short_tweet_text) #remove any outbound links\n",
        "  short_tweet_text_no_clause = remove_hanging_clause(short_tweet_tokens) #remove text at the end that does not form a whole sentence\n",
        "\n",
        "\n",
        "  return short_tweet_text_no_clause\n",
        "\n",
        "def shorten_output(text):\n",
        "  distribution = [1,1,1,1,1,1,1,1,1,1,1,2,2,2,3] #73 percent chance of selecting a tweet thread of length 1. 20 percent of length 2. ~7 percent for 3\n",
        "\n",
        "  max_tweet_length = int(280*random.choice(distribution)) #Sets the max length our generated text can be given a determined number of tweets in the thread\n",
        "  random_length_generator = random.randrange(30) #We don't want our tweets to always fill all 280 characters so we add a little more randomness to the length\n",
        "\n",
        "  return text[:max_tweet_length-random_length_generator] \n",
        "\n",
        "def remove_outbound_links_and_blank_tokens(text):\n",
        "  token_list = text.split(\" \")\n",
        "\n",
        "  for index,token in enumerate(token_list):  \n",
        "    if token[:12] == \"https://t.co\" or token[:11] == \"http://t.co\":\n",
        "      token_list.pop(index) #removes outbound links\n",
        "    elif token == \"\":\n",
        "      token_list.pop(index) #removes blank tokens\n",
        "    elif token == \"&amp;\": #output not recognizing ampersand\n",
        "      token_list[index] = \"&\"\n",
        "\n",
        "  return token_list\n",
        "\n",
        "def remove_hanging_clause(token_list): #Makes sure the tweet ends in a full sentence\n",
        "  latest_terminator_index = 0\n",
        "  #print(token_list)\n",
        "  for index,token in enumerate(token_list): \n",
        "    if token[-1] == \".\" or token[-1] == \"!\": #checks to see if a word ends in a terminator, used to keep the tweet to full sentences\n",
        "      latest_terminator_index = index\n",
        "\n",
        "  #print(latest_terminator_index)\n",
        "  shorten_tokens = token_list[:latest_terminator_index+1]\n",
        "  return \" \".join(shorten_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui1FOHBaEgWk",
        "colab_type": "text"
      },
      "source": [
        "Test some output!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZPac9A58LHF",
        "colab_type": "code",
        "outputId": "1f19d9b1-6618-4d10-8903-afdcf7ed02a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "output = gpt_predict(\"This is a witchhunt\")\n",
        "\n",
        "print(output)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'is', 'a', 'witchhunt!\\n“I’ve', 'had', 'a', 'very', 'good', 'meeting', 'with', 'Mitch', 'McConnell', 'and', 'the', 'Senate', 'Republicans.', 'A', 'lot', 'of', 'good', 'things', 'will', 'happen', 'on', 'our', 'agenda.”', '@McConnellsen', 'Just', 'spoke', 'to', 'Mitch', 'McConnell', 'and', 'others.', 'Big', 'win', 'on', 'Repeal', '&amp;', 'Replace.', 'We', 'will', 'get', 'it', 'done!\\n“The', 'R']\n",
            "39\n",
            "This is a witchhunt!\n",
            "“I’ve had a very good meeting with Mitch McConnell and the Senate Republicans. A lot of good things will happen on our agenda.” @McConnellsen Just spoke to Mitch McConnell and others. Big win on Repeal &amp; Replace.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8Kilc4UK2MR",
        "colab_type": "text"
      },
      "source": [
        "**Side note**: there's a few edge cases I haven't cleaned up yet, so the cell above may through an error every once in a while. Just run again if that happens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juOJpRMnEk5p",
        "colab_type": "text"
      },
      "source": [
        "### Make the bot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJnFkoxVEopa",
        "colab_type": "text"
      },
      "source": [
        "We define a function that will run in a loop. This runs the bot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42P0o02NGNu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prompt_predict():\n",
        "  \n",
        "  print(\"Hello! Let's get started\")\n",
        "  print(\"Input a sentence or two into the input field and watch the program complete the paragraph\")\n",
        "  print(\"\")\n",
        "  print(\"type 'quit' to exit the program\")\n",
        "\n",
        "  while True:\n",
        "\n",
        "    prompt = input(\"> \")\n",
        "    print(gpt_predict(prompt))\n",
        "\n",
        "    if prompt == \"quit\":\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tkFwSOjJf07",
        "colab_type": "text"
      },
      "source": [
        "Now have fun! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB2XUfmYJRBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prompt_predict()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
