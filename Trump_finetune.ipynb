{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trump_finetune.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsd19IPb5JJQ",
        "colab_type": "text"
      },
      "source": [
        "# Training a GPT-2 model on Donald Trump's tweets\n",
        "\n",
        "[GPT-2](https://openai.com/blog/better-language-models/) refers to a series of [transformer models](https://towardsdatascience.com/transformers-141e32e69591) developed by [OpenAI](https://towardsdatascience.com/transformers-141e32e69591) for automated text generation. \n",
        "\n",
        "\n",
        "GPT-2 comes pre-trained on text from eight million outbound links from Reddit. However, we can take this one step further and \"finetune\" a model with extra input from another source. This allows us to nudge the model to produce output more similar to this new text. For example, if you finetuned a GPT-2 model on \"The Great Gatsby\", it would pick up on common grammatical structures and might even start to wax longingly about Daisy Buchanan.  \n",
        "\n",
        "**In this demo we train the medium-sized GPT-2 model (355 million parameters) on over 28,000 tweets from the [@realDonaldTrump](https://twitter.com/realDonaldTrump) twitter account.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E41fJQUnAmtK",
        "colab_type": "text"
      },
      "source": [
        "The majority of the code for this demo is lifted (which much thanks) from a [colab notebook](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce?authuser=1#scrollTo=H7LoMj4GA4n_) put together by [Max Woolf](https://github.com/minimaxir), a data scientist at Buzzfeed.\n",
        "\n",
        "Max Woolf is also responsible for the [GPT-2-simple](https://github.com/minimaxir/gpt-2-simple) python library used in this demo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKDcZOHhqHF8",
        "colab_type": "text"
      },
      "source": [
        "This python notebook is intended for [Google Colab](https://colab.research.google.com/). Some commands may not work elsewhere."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysogDqsEGjf4",
        "colab_type": "text"
      },
      "source": [
        "## Set up coding environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HLr8oKtHOTG",
        "colab_type": "text"
      },
      "source": [
        "First In order to speed up training, make sure your colab runtime is using a GPU as its hardware accelerator. This will allow the model to finetune much faster. \n",
        "\n",
        "To tell colab to use a GPU, nagivate to the dropdown menu above labeled \"Runtime.\" Selecting \"Change runtime type\" opens up a window where you can select \"GPU\" as your \"Hardware accelerator.\" Hit \"SAVE\" and you are good to go. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38iSPZu3JP5Q",
        "colab_type": "text"
      },
      "source": [
        "Now, onto the code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWZq0-fV5B47",
        "colab_type": "code",
        "outputId": "ffc5afbc-ee61-4415-9975-6a267363946c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "!pip install gpt-2-simple #Installs gpt-2-simple library in your colab python environment\n",
        "import gpt_2_simple as gpt2 #imports the library for subsequent method calls\n",
        "from datetime import datetime \n",
        "from google.colab import files #module for uploading trump_tweets.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gpt-2-simple\n",
            "  Downloading https://files.pythonhosted.org/packages/75/2f/4b2d933decca7f79e3ae2eb3859e2b30bb1f572634d2c84f925d765e3b8e/gpt_2_simple-0.6.tar.gz\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.17.4)\n",
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2.8)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.6-cp36-none-any.whl size=25388 sha256=0a9bae74df1dc871e134c59c8a206431a1cec0e9dd684377fdfd4fcd42b629aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/e7/21/4cb10bcf085ff791a08bbd03aa3fd860f6e730f37b5dbbea28\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: regex, toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.6 regex-2019.11.1 toposort-1.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DHabfrBCttL",
        "colab_type": "text"
      },
      "source": [
        "Because gpt-2-simple uses an old version of tensorflow, we have to force colab to use an older release than tensorflow 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlVbxqH9CuN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ <= \"2.0\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjLjEhQ5GgP_",
        "colab_type": "text"
      },
      "source": [
        "## Upload Trump tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzbWn2ixGTEu",
        "colab_type": "text"
      },
      "source": [
        "Before we do anything with GPT-2, let's upload all of Trump's tweets into our local directory. \n",
        "\n",
        "Thankfully, gpt-2-simple is smart enough to be able to read a ***single column*** csv when finetuning. \n",
        "\n",
        "\n",
        "So, before making this notebook, I downloaded over 40,000 tweets from the @realdonaldtrump twitter account from [this website](http://www.trumptwitterarchive.com/archive). The site even has an option where you can select to download only the text of the tweets. I then removed all retweets (with the [remove_RT.py](https://github.com/aaronbrezel/GPT-2_Demo/blob/master/bot/remove_RT.py) file) from the corpus in order to avoid muddling our training data. We only want tweets written by the account.\n",
        "\n",
        "You can either collect and isolate the tweets yourself or use the csv from [this repository](https://github.com/aaronbrezel/GPT-2_Demo). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmK5xZOxNASc",
        "colab_type": "text"
      },
      "source": [
        "Download csv from Github repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCQkPrERM4yd",
        "colab_type": "code",
        "outputId": "7eb0b2a3-58d8-44c9-f426-265426771390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "!git clone https://github.com/aaronbrezel/GPT-2_Demo.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GPT-2_Demo'...\n",
            "remote: Enumerating objects: 2416, done.\u001b[K\n",
            "remote: Counting objects: 100% (2416/2416), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2018/2018), done.\u001b[K\n",
            "remote: Total 2433 (delta 397), reused 2404 (delta 389), pack-reused 17\u001b[K\n",
            "Receiving objects: 100% (2433/2433), 60.12 MiB | 9.96 MiB/s, done.\n",
            "Resolving deltas: 100% (402/402), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO3vvJw-O9H-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set variable that can easily find the csv of the text of Trump's tweets\n",
        "file_path_to_csv = '/content/GPT-2_Demo/bot/tweet_text_minus_rt.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP4i_bGQNNUC",
        "colab_type": "text"
      },
      "source": [
        "Upload csv from your local file system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smkkzsDhNQQp",
        "colab_type": "code",
        "outputId": "bd31f6cc-860f-4f8c-a23f-24c1b5deac0a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "# uploaded = files.upload()\n",
        "\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dbc9e282-adb6-48bb-ad45-962a19270853\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-dbc9e282-adb6-48bb-ad45-962a19270853\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZaOn-pZPcNM",
        "colab_type": "text"
      },
      "source": [
        "## GPT-2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwfyKmh1Pl7_",
        "colab_type": "text"
      },
      "source": [
        "Fetch the medium gpt-2 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Rq5B_KhPg_3",
        "colab_type": "code",
        "outputId": "921570fe-b00d-49a2-de72-8899b5bacaca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"355M\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 391Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 130Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 530Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:14, 97.2Mit/s]                                 \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 254Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 129Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 187Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYdTcupXP8NI",
        "colab_type": "text"
      },
      "source": [
        "Currently (Nov. 26, 2019), GPT-2-simple can only finetune on the small and medium GPT-2 models. \n",
        "\n",
        "To load the smaller model, change model_name from \"355M\" to \"124M\"\n",
        "\n",
        "You can also load the \"774M\" and \"1558M\" models. You cannot finetune them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9a4r-1tQ5z6",
        "colab_type": "text"
      },
      "source": [
        "### Start finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7vcfS0uQ8XV",
        "colab_type": "text"
      },
      "source": [
        "The following text cell is lifted directly from Max Woolf's GPT-2 [colab tutorial](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=LdpZQXknFNY3). Honestly, it was so short and to the point, that I didn't see a reason to change it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPXKyAURTJ9b",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "[gpt2.finetune()] will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. \n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMy6FMo6VtYJ",
        "colab_type": "text"
      },
      "source": [
        "Before training the model, make sure your csv has text entries in every row. If you take the csv from the github repo, you should not have this problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_CAkb_wUO5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "with open(file_path_to_csv) as fp:\n",
        "  reader = csv.reader(fp)\n",
        "  count = 0\n",
        "  for row in reader:\n",
        "    count = count + 1 \n",
        "    if len(row) != 0 and type(row[0]) == str: #checks that there is something in each row of the csv and that thing is a string\n",
        "      temp = \"everything is okay\"\n",
        "    else:\n",
        "      print(count) #Print the offending row index in the csv\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-G8MJEGXncS",
        "colab_type": "text"
      },
      "source": [
        "Now, we train. The more steps you pass, the more tightly we will fit to @realDonaldTrump's tweets. I usually like to err on less steps, as I find it makes the model more unpredictable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jemf1N0OQzs-",
        "colab_type": "code",
        "outputId": "871f2e63-5a5b-4dee-c529-fbb78ec2c2d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_path_to_csv,\n",
        "              model_name='355M',\n",
        "              steps=400,\n",
        "              restore_from='fresh',\n",
        "              run_name='trump_tune_small',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500,\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint models/355M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 12.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 1242687 tokens\n",
            "Training...\n",
            "[10 | 17.36] loss=2.65 avg=2.65\n",
            "[20 | 26.12] loss=2.44 avg=2.55\n",
            "[30 | 34.91] loss=2.86 avg=2.65\n",
            "[40 | 43.67] loss=2.55 avg=2.63\n",
            "[50 | 52.44] loss=2.18 avg=2.53\n",
            "[60 | 61.20] loss=1.93 avg=2.43\n",
            "[70 | 69.97] loss=2.28 avg=2.41\n",
            "[80 | 78.73] loss=1.71 avg=2.32\n",
            "[90 | 87.50] loss=1.94 avg=2.27\n",
            "[100 | 96.25] loss=2.01 avg=2.25\n",
            "[110 | 105.00] loss=2.06 avg=2.23\n",
            "[120 | 113.76] loss=1.80 avg=2.19\n",
            "[130 | 122.50] loss=1.99 avg=2.18\n",
            "[140 | 131.25] loss=2.13 avg=2.17\n",
            "[150 | 140.01] loss=2.64 avg=2.21\n",
            "[160 | 148.76] loss=2.13 avg=2.20\n",
            "[170 | 157.51] loss=1.72 avg=2.17\n",
            "[180 | 166.26] loss=2.00 avg=2.16\n",
            "[190 | 175.01] loss=2.58 avg=2.18\n",
            "[200 | 183.76] loss=2.77 avg=2.22\n",
            "======== SAMPLE 1 ========\n",
            "24|<|startoftext|>@Wes_Haley Thanks. It is worth the wait.<|endoftext|>\n",
            "<|startoftext|>@D_G_Stellar    Thank you.<|endoftext|>\n",
            "<|startoftext|>On my way @JaredFlynn and I discussed a few of the issues facing our great nation. Jared is the best. We agree. We want a great &amp; powerful @USNavy. Thanks.<|endoftext|>\n",
            "<|startoftext|>@mrb2                        Thank you.<|endoftext|>\n",
            "<|startoftext|>@gw_davis                   @JIMJOSÉ                You know it &amp; will be great.<|endoftext|>\n",
            "<|startoftext|>@bretriss       My husband is great! He is very nice and smart--he has a lot of heart! Thanks.<|endoftext|>\n",
            "<|startoftext|>@kimweschrader @bretriss  If she doesn’t win it will be a great day!<|endoftext|>\n",
            "<|startoftext|>@nh_walls                           was so nice and professional!  I will never think of the other guys with nothing.<|endoftext|>\n",
            "<|startoftext|>It has just been another wonderful day for the NFL! @NFLPA  Good luck I like you! @BearsNetwork Thanks.<|endoftext|>\n",
            "<|startoftext|>@M_T_McBride                                         \"JOSHUA: A BIGGER AND LOUDER THAN EVER BEFORE\" The NFL should take the team to the Super Bowl.<|endoftext|>\n",
            "<|startoftext|>@tommat_mcintyre                        \"@JoeDoyle                  \"We must not let this happen again. We must keep fighting!\"<|endoftext|>\n",
            "<|startoftext|>@CaraO @britishem_scully                             \"I am glad my family is still alive.<|endoftext|>\n",
            "<|startoftext|>Wow it was nice of @britishemsports to give the #USGA award to the people it has always been intended for. I loved that!<|endoftext|>\n",
            "<|startoftext|><|startoftext|>@jleeper                 \"The only way the media knows the truth   is if you challenge them. What a waste.\"  -- Benjamin Franklin.   -- http://t.co/lF6fTjK8Pn1   \n",
            "<|startoftext|>This country needs to get its act together!  http://t.co/tYWG2G3iDU<|endoftext|>\n",
            "<|startoftext|>I'm on my way to Washington D.C. but first I have to get to Hawaii! If the @Gf_Catherine_McHale is the woman of the year then she is my first choice.<|endoftext|>\n",
            "<|startoftext|>@Bjurrenfelz                               The only one who talks is you!<|endoftext|>\n",
            "<|startoftext|>The media is very upset at the fact that when Trump is winning the ratings are going down when Hillary is winning the polls.  People are just not watching  and if it were up to the pundits they may have a field day.<|endoftext|>\n",
            "<|startoftext|>Thank you Bill to @b\n",
            "\n",
            "[210 | 211.13] loss=2.47 avg=2.23\n",
            "[220 | 219.90] loss=1.66 avg=2.20\n",
            "[230 | 228.64] loss=2.15 avg=2.20\n",
            "[240 | 237.38] loss=2.00 avg=2.19\n",
            "[250 | 246.14] loss=1.63 avg=2.16\n",
            "[260 | 254.90] loss=2.23 avg=2.17\n",
            "[270 | 263.66] loss=1.67 avg=2.15\n",
            "[280 | 272.42] loss=2.16 avg=2.15\n",
            "[290 | 281.18] loss=1.87 avg=2.14\n",
            "[300 | 289.92] loss=1.95 avg=2.13\n",
            "[310 | 298.68] loss=1.91 avg=2.12\n",
            "[320 | 307.44] loss=1.80 avg=2.11\n",
            "[330 | 316.20] loss=1.84 avg=2.10\n",
            "[340 | 324.96] loss=2.48 avg=2.11\n",
            "[350 | 333.73] loss=1.60 avg=2.09\n",
            "[360 | 342.50] loss=1.84 avg=2.09\n",
            "[370 | 351.26] loss=1.84 avg=2.08\n",
            "[380 | 360.02] loss=2.10 avg=2.08\n",
            "[390 | 368.77] loss=1.73 avg=2.07\n",
            "[400 | 377.53] loss=2.08 avg=2.07\n",
            "Saving checkpoint/trump_tune_small/model-400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw_3AToIk4yy",
        "colab_type": "text"
      },
      "source": [
        "Test it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtgEfc6ak7Z6",
        "colab_type": "code",
        "outputId": "79f1a077-a335-4a5f-b0bc-120f1068edd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "gpt2.generate(sess, length=50, temperature=0.9, prefix=\"Nancy Pelosi is cool!\", run_name='trump_tune_small')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nancy Pelosi is cool!<|endoftext|>\n",
            "<|startoftext|>Thank you South Florida for the group of gentlemen in front of the statue of Thomas Jefferson. They all deserve a good (very) sincere reply.<|endoftext|>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgSOz-D0wT_M",
        "colab_type": "text"
      },
      "source": [
        "Pretty neat, although we have a few text artifacts from the training left over. We can clean these up quite a bit in the next tutorial where we create a simple bot loop for prompting tweets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LxRSkI4rcNx",
        "colab_type": "text"
      },
      "source": [
        "### Mount gdrive and save model\n",
        "\n",
        "Finetuning a GPT-2 model creates a pretty big folder. You can save it on your local machine, but since we're in colab, we might as well save it on drive. In the code snipits below, we \"mount\" your gdrive, which allows you to read and write files from your drive directly in colab. We then use a handy method from the gpt-2-simple library to copy the information from our new model into gdrive. The .tar file we save is roughly 1.3GB\n",
        "\n",
        "We can then access the finetuned model anytime from this file, without having to retrain. Very convenient.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqMl2yP3l8ib",
        "colab_type": "code",
        "outputId": "444b6edd-5798-44ef-90f4-c2898f5a093b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #For the record, there is also a method in gpt-2-simple that does this, but the same action"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHpVfMOnl9_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='trump_tune_small')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8XyvNw2xElX",
        "colab_type": "text"
      },
      "source": [
        "### Check out the trump_demo.ipynb file in the repository for the next step."
      ]
    }
  ]
}
